{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import src.helpers.metrics as evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the path to the file\n",
    "\n",
    "path = 'trained_models/'\n",
    "version = path + '18_06_2024__13-10'\n",
    "\n",
    "dataset = 'student_addiction'\n",
    "\n",
    "# Load the models\n",
    "\n",
    "for model in ['rf']: #['rf', 'rbf_svm', 'dnn']:\n",
    "    \n",
    "    # Load the model\n",
    "    loaded_model = joblib.load(version + '/' + dataset + '_' + model + '.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimentation                       int64\n",
      "Academic_Performance_Decline          int64\n",
      "Social_Isolation                      int64\n",
      "Financial_Issues                      int64\n",
      "Physical_Mental_Health_Problems       int64\n",
      "Legal_Consequences                    int64\n",
      "Relationship_Strain                   int64\n",
      "Risk_Taking_Behavior                  int64\n",
      "Withdrawal_Symptoms                   int64\n",
      "Denial_and_Resistance_to_Treatment    int64\n",
      "Addiction_Class                       int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import src.data.api.models as data_models\n",
    "import dice_ml\n",
    "from dice_ml.utils import helpers  # helper functions\n",
    "from dice_ml import Dice\n",
    "\n",
    "\n",
    "student = data_models.StudentAddiction()\n",
    "\n",
    "\n",
    "df = student.get_data()\n",
    "\n",
    "#print dtypes\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30072\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = student.train_test_split(student.getX(), student.getY())\n",
    "\n",
    "Xy_train_merged = X_train.copy()\n",
    "Xy_train_merged[student.y] = y_train\n",
    "\n",
    "print(len(Xy_train_merged))\n",
    "\n",
    "d = dice_ml.Data(dataframe=df, continuous_features=[], outcome_name='Addiction_Class')\n",
    "\n",
    "# Experimentation                       int64\n",
    "# Academic_Performance_Decline          int64\n",
    "# Social_Isolation                      int64\n",
    "# Financial_Issues                      int64\n",
    "# Physical_Mental_Health_Problems       int64\n",
    "# Legal_Consequences                    int64\n",
    "# Relationship_Strain                   int64\n",
    "# Risk_Taking_Behavior                  int64\n",
    "# Withdrawal_Symptoms                   int64\n",
    "# Denial_and_Resistance_to_Treatment    int64\n",
    "# Addiction_Class                       int64\n",
    "# dtype: object\n",
    "\n",
    "deeeed = dice_ml.data.Data(features={\n",
    "            'Experimentation': [0,1],\n",
    "            'Academic_Performance_Decline': [0,1],\n",
    "            'Social_Isolation': [0,1],\n",
    "            'Financial_Issues': [0,1],\n",
    "            'Physical_Mental_Health_Problems': [0,1],\n",
    "            'Legal_Consequences': [0,1],\n",
    "            'Relationship_Strain': [0,1],\n",
    "            'Risk_Taking_Behavior': [0,1],\n",
    "            'Withdrawal_Symptoms': [0,1],\n",
    "            'Denial_and_Resistance_to_Treatment': [0,1]},\n",
    "         outcome_name='Addiction_Class',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diverse Counterfactuals found! total time taken: 00 min 00 sec\n",
      "Query instance (original outcome : 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experimentation</th>\n",
       "      <th>Academic_Performance_Decline</th>\n",
       "      <th>Social_Isolation</th>\n",
       "      <th>Financial_Issues</th>\n",
       "      <th>Physical_Mental_Health_Problems</th>\n",
       "      <th>Legal_Consequences</th>\n",
       "      <th>Relationship_Strain</th>\n",
       "      <th>Risk_Taking_Behavior</th>\n",
       "      <th>Withdrawal_Symptoms</th>\n",
       "      <th>Denial_and_Resistance_to_Treatment</th>\n",
       "      <th>Addiction_Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Experimentation  Academic_Performance_Decline  Social_Isolation  \\\n",
       "0                1                             1                 1   \n",
       "\n",
       "   Financial_Issues  Physical_Mental_Health_Problems  Legal_Consequences  \\\n",
       "0                 0                                1                   0   \n",
       "\n",
       "   Relationship_Strain  Risk_Taking_Behavior  Withdrawal_Symptoms  \\\n",
       "0                    0                     0                    1   \n",
       "\n",
       "   Denial_and_Resistance_to_Treatment  Addiction_Class  \n",
       "0                                   0                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diverse Counterfactual set (new outcome: 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experimentation</th>\n",
       "      <th>Academic_Performance_Decline</th>\n",
       "      <th>Social_Isolation</th>\n",
       "      <th>Financial_Issues</th>\n",
       "      <th>Physical_Mental_Health_Problems</th>\n",
       "      <th>Legal_Consequences</th>\n",
       "      <th>Relationship_Strain</th>\n",
       "      <th>Risk_Taking_Behavior</th>\n",
       "      <th>Withdrawal_Symptoms</th>\n",
       "      <th>Denial_and_Resistance_to_Treatment</th>\n",
       "      <th>Addiction_Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Experimentation Academic_Performance_Decline Social_Isolation  \\\n",
       "0               -                            -                -   \n",
       "\n",
       "  Financial_Issues Physical_Mental_Health_Problems Legal_Consequences  \\\n",
       "0                -                               -                  -   \n",
       "\n",
       "  Relationship_Strain Risk_Taking_Behavior Withdrawal_Symptoms  \\\n",
       "0                   -                    -                   -   \n",
       "\n",
       "  Denial_and_Resistance_to_Treatment Addiction_Class  \n",
       "0                                  -               -  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = dice_ml.Model(model=loaded_model, backend='sklearn')\n",
    "exp = dice_ml.Dice(d, m, method='random')\n",
    "\n",
    "query_instance = X_test[0:1]\n",
    "dice_exp = exp.generate_counterfactuals(query_instance, total_CFs=1, desired_class=1, verbose=True,\n",
    "                                        diversity_weight=1,\n",
    "                                        proximity_weight=1,\n",
    "                                        sparsity_weight=2)\n",
    "\n",
    "# Visualize counterfactual explanation\n",
    "dice_exp.visualize_as_dataframe(show_only_changes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    20953\n",
      "1     9119\n",
      "Name: Addiction_Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# count class imbalance\n",
    "\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = evaluation.CounterfactualAnalysis(model='DICE', factual=query_instance, counterfactual=dice_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.helpers.helpers as helpers\n",
    "import src.helpers.metrics as evaluation\n",
    "import src.data.api.models as data_models\n",
    "import src.cf_methods.models as cf_algs\n",
    "import src.helpers.gridsearch as gridsearch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datasets = helpers.load_json_file('training_settings.json')\n",
    "params = helpers.load_json_file('experiment_params.json')\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "#Repeat for all active datasets\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    dataset_results = None\n",
    "    \n",
    "    if datasets[dataset]['active'] == False:\n",
    "        continue\n",
    "    \n",
    "    log = f\"CF evaluation for {dataset}\"\n",
    "    \n",
    "    dataset_model_name = datasets[dataset]['data_model']\n",
    "    dataset_model_class = getattr(data_models, dataset_model_name, None)\n",
    "    \n",
    "    if dataset_model_name == None:\n",
    "        print(f\"Dataset {dataset} has no model name\")\n",
    "        continue\n",
    "    \n",
    "    if dataset_model_class is not None:\n",
    "        \n",
    "        # Load the dataset with the smote parameter (e.g. False or 'auto')and random state\n",
    "        \n",
    "        smote = datasets[dataset]['smote']\n",
    "        cleaned_data = dataset_model_class(smote=smote, random_state=random_state)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Dataset {dataset} ({dataset_model_name}) not found\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # Split the data into target variable and features\n",
    "    \n",
    "    X = cleaned_data['X']\n",
    "    y = cleaned_data['y']\n",
    "        \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = cleaned_data.train_test_split(X, y)\n",
    "    \n",
    "\n",
    "    # Repeat for all models\n",
    "    \n",
    "    models = [\n",
    "        'NICE_minmax', 'NICE_standard'\n",
    "        'DICE_random', 'DICE_genetic', 'DICE_kdtree',\n",
    "        'CE-OCL', 'CE-OCL_trust'\n",
    "        ]\n",
    "    \n",
    "    experiments = [\"diversity-proximity\", \"sparsity-proximity\", \"computational-efficiency\"]\n",
    "    \n",
    "    for experiment in experiments:\n",
    "    \n",
    "        for model in models: \n",
    "        \n",
    "        # Set FIXED parameters for the model\n",
    "        \n",
    "        \n",
    "            match model:\n",
    "                \n",
    "                case 'NICE_minmax' | 'NICE_standard':\n",
    "                    \n",
    "                    params = params[experiment]['NICE']\n",
    "                    \n",
    "                case 'DICE_random' | 'DICE_genetic' | 'DICE_kdtree':\n",
    "                    \n",
    "                    params = params[experiment]['DICE']\n",
    "                \n",
    "                \n",
    "                case 'CE_OCL' | 'CE_OCL_trust':\n",
    "                    \n",
    "                    params = params[experiment]['CE_OCL']\n",
    "                    \n",
    "                    model_instance = cf_algs.CE_OCL(X_train)\n",
    "                    \n",
    "                    model_instance.set_data_restrictions(\n",
    "                    real_features = [],\n",
    "                    binary_features = [],\n",
    "                    integer_features = [],\n",
    "                    categorical_encodings = {},\n",
    "                    \n",
    "                    # Empty in order to turn off actionability constraints\n",
    "                    only_positive_features = [],\n",
    "                    only_increasing_features = [],\n",
    "                    immutable_features = [],\n",
    "                    conditionally_mutable_features = []\n",
    "                    )\n",
    "                    \n",
    "                    # Trust regions require at least 1 observation in the target class (of the COUNTERfactual)\n",
    "                    ref = np.where(y_test==1)\n",
    "                    \n",
    "                    model_instance.set_metric_constraints(\n",
    "                        sparsity_constraint = False,\n",
    "                        trust_region_constraint = False,\n",
    "                        trust_region_reference=None,\n",
    "                    )\n",
    "            \n",
    "                case _:\n",
    "                    raise ValueError('Model not supported')\n",
    "            \n",
    "            \n",
    "            # Set the scoring function for the grid search\n",
    "            if experiment == \"diversity-proximity\":\n",
    "            \n",
    "                def custom_scoring(metrics):\n",
    "                    return ( metrics['diversity'] + metrics['proximity']) / 2\n",
    "                \n",
    "            elif experiment == \"sparsity-proximity\":\n",
    "                \n",
    "                def custom_scoring(metrics):\n",
    "                    return ( metrics['sparsity'] + metrics['proximity'] ) / 2\n",
    "                \n",
    "            search = gridsearch.GridSearch(model=model_instance,\n",
    "                                        param_grid=params,\n",
    "                                        scoring=custom_scoring, # can also be 'diversity', 'proximity', 'sparsity',....\n",
    "                                        )      \n",
    "            \n",
    "            # Find optimal parameters using grid search on the TEST dataset\n",
    "            search.optimize(X_test, y_test)\n",
    "            \n",
    "            best_params = search.self.best_params_\n",
    "            \n",
    "            # Print best params using some nice markup  and ###\n",
    "            \n",
    "            print(f\"Best parameters for {model} in {dataset} given scoring function '{scoring}' are:\")\n",
    "            print(f\"{'#'*20}\")\n",
    "            print(best_params)\n",
    "            \n",
    "            analysis = evaluation.CounterfactualAnalysis(model_instance, dataset)\n",
    "            metrics = analysis.evaluate()\n",
    "            \n",
    "            if dataset_results is None:\n",
    "                dataset_results = metrics\n",
    "            else:\n",
    "                dataset_results = pd.concat([dataset_results, metrics])\n",
    "            \n",
    "    #Save results\n",
    "    dataset_results.to_csv(f\"output/results/{experiment}-{dataset}_results.csv\")\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
