{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import all datasets, helpers\n",
    "\n",
    "import src.data.api.models as data_models\n",
    "import src.helpers.helpers as helpers\n",
    "\n",
    "# Import all the necessary libraries\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from dask.distributed import Client, LocalCluster \n",
    "import os\n",
    "\n",
    "output = 'trained_models' # Output folder for trained models\n",
    "random_state = 42 # Random state for reproducibility\n",
    "n_jobs = 4 # Number of cores to use for parallel processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here it is possible to adjust the datasets and models that will be used in the project\n",
    "# Set any of the pred_models to None if you don't want to use it\n",
    "\n",
    "# Load datasets from training_settings.json\n",
    "\n",
    "datasets = helpers.load_json_file('training_settings.json')\n",
    "\n",
    "# e.g. if you want to remove the random_forest model from the breast_cancer dataset\n",
    "# datasets['breast_cancer']['pred_models']['random_forest'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running rf on credit_score\n",
      "Fitting 2 folds for each of 135 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 275.45 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "2024-06-25 22:47:36,499 - distributed.worker - ERROR - failed during get data with tcp://127.0.0.1:61089 -> tcp://127.0.0.1:61090\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/leonvr/Library/Python/3.11/lib/python/site-packages/tornado/iostream.py\", line 962, in _handle_write\n",
      "    num_bytes = self.write_to_fd(self._write_buffer.peek(size))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/leonvr/Library/Python/3.11/lib/python/site-packages/tornado/iostream.py\", line 1124, in write_to_fd\n",
      "    return self.socket.send(data)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [Errno 55] No buffer space available\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/worker.py\", line 1782, in get_data\n",
      "    response = await comm.read(deserializers=serializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:61089 remote=tcp://127.0.0.1:61108>: OSError: [Errno 55] No buffer space available\n",
      "2024-06-25 22:47:36,504 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:61089\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/worker.py\", line 2058, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:61108 remote=tcp://127.0.0.1:61089>: Stream is closed\n",
      "2024-06-25 22:55:23,383 - distributed.scheduler - WARNING - Worker failed to heartbeat for 403s; attempting restart: <WorkerState 'tcp://127.0.0.1:61090', name: 2, status: running, memory: 16, processing: 48>\n",
      "2024-06-25 22:55:27,469 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-06-25 22:55:27,544 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:61090\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/leonvr/Library/Python/3.11/lib/python/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/leonvr/Library/Python/3.11/lib/python/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/worker.py\", line 2058, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:61151 remote=tcp://127.0.0.1:61090>: ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "2024-06-25 22:55:27,545 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:61090\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/leonvr/Library/Python/3.11/lib/python/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/leonvr/Library/Python/3.11/lib/python/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/worker.py\", line 2058, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:61119 remote=tcp://127.0.0.1:61090>: ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "2024-06-25 22:55:27,560 - distributed.scheduler - ERROR - Removing worker 'tcp://127.0.0.1:61090' caused the cluster to lose scattered data, which can't be recovered: {'ndarray-6b982a733e224f75a328033d5f9a7a13', 'ndarray-3a7385b50c4f40909e2f8f62110b395e'} (stimulus_id='handle-worker-cleanup-1719348927.559909')\n",
      "2024-06-25 22:55:27,567 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:61090\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/leonvr/Library/Python/3.11/lib/python/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/leonvr/Library/Python/3.11/lib/python/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/worker.py\", line 2058, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:61121 remote=tcp://127.0.0.1:61090>: ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "2024-06-25 22:55:35,893 - distributed.nanny - WARNING - Restarting worker\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "1 fits failed out of a total of 270.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 489, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 67, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py\", line 1952, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py\", line 1595, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py\", line 1718, in _retrieve\n",
      "    for result in batched_results:\n",
      "TypeError: 'CancelledError' object is not iterable\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.71703756 0.72115702 0.71506733 0.71561359 0.72008237 0.71806743\n",
      " 0.72061074 0.71830922 0.71687636 0.71855999 0.7169838  0.71547037\n",
      " 0.71235388 0.71720771 0.71724353 0.71880174 0.71513901 0.71861367\n",
      " 0.71588231 0.717342   0.71991223 0.71675104 0.71407334 0.71906149\n",
      " 0.72015401 0.71768232 0.71833608 0.72966461 0.72970939 0.729987\n",
      " 0.73000492 0.72989745 0.72962879 0.72999596 0.72976312 0.73003178\n",
      " 0.72939595 0.73015715 0.73009447 0.72889444 0.72997805 0.72979894\n",
      " 0.73016611 0.73004074 0.72991536 0.72923475 0.73013029 0.73005865\n",
      " 0.73026462 0.7298258  0.72984372 0.72985267 0.72968253 0.7298258\n",
      " 0.73515424 0.73544081 0.73597814 0.73587963 0.73509156 0.73707067\n",
      " 0.73438408 0.73626469 0.73581694 0.73531543 0.73408856 0.73569157\n",
      " 0.73499304 0.733999   0.73493035 0.73569157 0.73451841 0.73515423\n",
      " 0.73509155 0.73550352 0.7348229  0.73426766 0.73636322 0.73623784\n",
      " 0.73760802 0.73611245 0.73536917 0.74403795 0.74574842 0.74594545\n",
      " 0.745032   0.74495141 0.74509468 0.74482602 0.74296332 0.74414543\n",
      " 0.74728875 0.74325885 0.74513947 0.74410959 0.74647382 0.74565888\n",
      " 0.74541708 0.74664397 0.74502304 0.74530963 0.74483498 0.74556933\n",
      " 0.74547083 0.74553351 0.74613351 0.74514843 0.74595441 0.74411856\n",
      "        nan 0.79982087 0.80025073 0.79574618 0.79623873 0.798191\n",
      " 0.79053416 0.79056997 0.79096401 0.79372226 0.79535214 0.79571037\n",
      " 0.79358794 0.79404467 0.79533423 0.7882595  0.78933414 0.78927145\n",
      " 0.78499976 0.78631619 0.78723859 0.78614603 0.78645052 0.78632514\n",
      " 0.78561768 0.78439974 0.78506244]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running rbf_svm on credit_score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 61324 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 16 candidates, totalling 32 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 26.20 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "2024-06-26 06:07:53,050 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 1 memory: 32 MB fds: 71>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/leonvr/Library/Python/3.11/lib/python/site-packages/tornado/ioloop.py\", line 919, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/system_monitor.py\", line 168, in update\n",
      "    net_ioc = psutil.net_io_counters()\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/psutil/__init__.py\", line 2122, in net_io_counters\n",
      "    rawdict = _psplatform.net_io_counters()\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [Errno 12] Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running dnn on credit_score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 64074 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######################################\n",
    "### Run the models on the datasets ###\n",
    "######################################\n",
    "\n",
    "# Loop through all the datasets\n",
    "\n",
    "# Record the current date and time to the nearest minute\n",
    "now_format = datetime.now().strftime(\"%d_%m_%Y__%H-%M\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    if datasets[dataset]['active'] == False:\n",
    "        continue\n",
    "    \n",
    "    log = f\"Model training evaluation for {dataset} on {now_format}\"\n",
    "    \n",
    "    \n",
    "    # Get the model name from the dataset and load it, else set it to None\n",
    "    \n",
    "    \n",
    "    dataset_model_name = datasets[dataset]['data_model']\n",
    "    dataset_model_class = getattr(data_models, dataset_model_name, None)\n",
    "    \n",
    "    if dataset_model_name == None:\n",
    "        print(f\"Dataset {dataset} has no model name\")\n",
    "        continue\n",
    "    \n",
    "    if dataset_model_class is not None:\n",
    "        \n",
    "        # Load the dataset with the smote parameter (e.g. False or 'auto')and random state\n",
    "        \n",
    "        smote = datasets[dataset]['smote']\n",
    "        crossval = datasets[dataset]['cross_validation']\n",
    "        cleaned_data = dataset_model_class(smote=smote, random_state=random_state)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Dataset {dataset} ({dataset_model_name}) not found\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # Split the data into target variable and featurest\n",
    "    \n",
    "    X = cleaned_data['X']\n",
    "    y = cleaned_data['y']\n",
    "        \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = cleaned_data.train_test_split(X, y)\n",
    "    \n",
    "    \n",
    "    # Train all the models on the dataset\n",
    "    for pred_model in datasets[dataset]['pred_models']:\n",
    "        \n",
    "        print(f\"Running {pred_model} on {dataset}\")\n",
    "        \n",
    "        # Set the steps and parameters for the pipeline\n",
    "        pipe_params = {}\n",
    "\n",
    "        match pred_model:\n",
    "            \n",
    "            case 'rf':\n",
    "                steps = [('rf', RandomForestClassifier())]\n",
    "                pipe_params.update({\n",
    "                    'rf__n_estimators': [100, 200, 300],\n",
    "                    'rf__max_depth': [2, 4, 8, 10, 20],\n",
    "                    'rf__min_samples_split': [2, 5, 10],\n",
    "                    'rf__min_samples_leaf': [1, 2, 4]\n",
    "                })\n",
    "                \n",
    "            case 'rbf_svm':\n",
    "                steps = [\n",
    "                        ('scaler', StandardScaler()),\n",
    "                        ('rbf_svm', SVC())]\n",
    "                pipe_params.update({\n",
    "                    'rbf_svm__C': [0.1, 1, 10, 100],\n",
    "                    'rbf_svm__gamma': [1, 0.1, 0.01, 0.001],\n",
    "                    'rbf_svm__kernel': ['rbf']\n",
    "                })\n",
    "                \n",
    "            case 'dnn':\n",
    "                steps = [('dnn', MLPClassifier())]\n",
    "                pipe_params.update({\n",
    "                    'dnn__hidden_layer_sizes': [(100,)],\n",
    "                    'dnn__activation': ['relu', 'tanh'],\n",
    "                    'dnn__alpha': [0.0001, 0.001, 0.01],\n",
    "                    'dnn__learning_rate': ['constant', 'adaptive']\n",
    "                })\n",
    "                    \n",
    "                \n",
    "            case _:\n",
    "                steps = None\n",
    "                print(f\"Model {pred_model} not found\")\n",
    "                \n",
    "        # Apply override parameters\n",
    "        if datasets[dataset]['pred_models'][pred_model] is not None or datasets[dataset]['pred_models'][pred_model] != {}:\n",
    "            pipe_params.update(datasets[dataset]['pred_models'][pred_model])\n",
    "                \n",
    "        pipe = Pipeline(steps=steps)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        search = GridSearchCV(pipe, pipe_params, cv=crossval, n_jobs=n_jobs, verbose=1)\n",
    "        \n",
    "        cluster = LocalCluster()  \n",
    "        client = Client(cluster) \n",
    "        \n",
    "        with joblib.parallel_backend(\"dask\", scatter=[X_train, y_train]):  \n",
    "            search.fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "        # Create a folder with the datetime if it doesn't exist\n",
    "        base = output + '/' + now_format\n",
    "        if not os.path.exists(base):\n",
    "            os.makedirs(base)\n",
    "        \n",
    "        # Save the model to the output folder with the dataset name and the model name\n",
    "        joblib.dump(search.best_estimator_, f\"{base}/{dataset}_{pred_model}.joblib\")\n",
    "        \n",
    "        # Log the evaluation metrics to a log file, including the best parameters\n",
    "        log += helpers.dataset_log(search, X_test, y_test, pred_model, cleaned_data.get_labels())\n",
    "    \n",
    "        \n",
    "    # Save the log to a file\n",
    "    with open(f\"{base}/{dataset}_model_comparison.txt\", 'w') as file:\n",
    "        file.write(log)\n",
    "        file.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
