{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import all datasets, helpers\n",
    "\n",
    "import src.data.api.models as data_models\n",
    "import src.helpers.helpers as helpers\n",
    "\n",
    "# Import all the necessary libraries\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from dask.distributed import Client, LocalCluster \n",
    "import os\n",
    "\n",
    "output = 'trained_models' # Output folder for trained models\n",
    "random_state = 42 # Random state for reproducibility\n",
    "n_jobs = 4 # Number of cores to use for parallel processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here it is possible to adjust the datasets and models that will be used in the project\n",
    "# Set any of the pred_models to None if you don't want to use it\n",
    "\n",
    "# Load datasets from training_settings.json\n",
    "\n",
    "datasets = helpers.load_json_file('training_settings.json')\n",
    "\n",
    "# e.g. if you want to remove the random_forest model from the breast_cancer dataset\n",
    "# datasets['breast_cancer']['pred_models']['random_forest'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running rf on credit_score\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "To use Joblib with Dask first create a Dask Client\n\n    from dask.distributed import Client\n    client = Client()\nor\n    client = Client('scheduler-address:8786')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/_dask.py:164\u001b[0m, in \u001b[0;36mDaskDistributedBackend.__init__\u001b[0;34m(self, scheduler_host, scatter, client, loop, wait_for_workers_timeout, **submit_kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43mget_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/distributed/worker.py:2799\u001b[0m, in \u001b[0;36mget_client\u001b[0;34m(address, timeout, resolve_address)\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2799\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo global client found and no address provided\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: No global client found and no address provided",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 101\u001b[0m\n\u001b[1;32m     97\u001b[0m pipe\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     99\u001b[0m search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipe, pipe_params, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:  \n\u001b[1;32m    102\u001b[0m     search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Create a folder with the datetime if it doesn't exist\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:534\u001b[0m, in \u001b[0;36mparallel_backend.__init__\u001b[0;34m(self, backend, n_jobs, inner_max_num_threads, **backend_params)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, inner_max_num_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    532\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbackend_params):\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m        \u001b[49m\u001b[43minner_max_num_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_max_num_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbackend_params\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_parallel_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_backend_and_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:366\u001b[0m, in \u001b[0;36mparallel_config.__init__\u001b[0;34m(self, backend, n_jobs, verbose, temp_folder, max_nbytes, mmap_mode, prefer, require, inner_max_num_threads, **backend_params)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    349\u001b[0m     backend\u001b[38;5;241m=\u001b[39mdefault_parallel_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    360\u001b[0m ):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# Save the parallel info and set the active parallel config\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_parallel_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m    363\u001b[0m         _backend, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m, default_parallel_config\n\u001b[1;32m    364\u001b[0m     )\n\u001b[0;32m--> 366\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_max_num_threads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbackend_params\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m     new_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: n_jobs,\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m: verbose,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend\u001b[39m\u001b[38;5;124m\"\u001b[39m: backend\n\u001b[1;32m    379\u001b[0m     }\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_parallel_config\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:418\u001b[0m, in \u001b[0;36mparallel_config._check_backend\u001b[0;34m(self, backend, inner_max_num_threads, **backend_params)\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    413\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    414\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid backend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, expected one of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    415\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(BACKENDS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    416\u001b[0m             )\n\u001b[0;32m--> 418\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[43mBACKENDS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbackend_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_max_num_threads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner_max_num_threads argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/_dask.py:172\u001b[0m, in \u001b[0;36mDaskDistributedBackend.__init__\u001b[0;34m(self, scheduler_host, scatter, client, loop, wait_for_workers_timeout, **submit_kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    166\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use Joblib with Dask first create a Dask Client\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    from dask.distributed import Client\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    client = Client()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    client = Client(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscheduler-address:8786\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m client\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scatter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scatter, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "\u001b[0;31mValueError\u001b[0m: To use Joblib with Dask first create a Dask Client\n\n    from dask.distributed import Client\n    client = Client()\nor\n    client = Client('scheduler-address:8786')"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######################################\n",
    "### Run the models on the datasets ###\n",
    "######################################\n",
    "\n",
    "# Loop through all the datasets\n",
    "\n",
    "# Record the current date and time to the nearest minute\n",
    "now_format = datetime.now().strftime(\"%d_%m_%Y__%H-%M\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    if datasets[dataset]['active'] == False:\n",
    "        continue\n",
    "    \n",
    "    log = f\"Model training evaluation for {dataset} on {now_format}\"\n",
    "    \n",
    "    \n",
    "    # Get the model name from the dataset and load it, else set it to None\n",
    "    \n",
    "    \n",
    "    dataset_model_name = datasets[dataset]['data_model']\n",
    "    dataset_model_class = getattr(data_models, dataset_model_name, None)\n",
    "    \n",
    "    if dataset_model_name == None:\n",
    "        print(f\"Dataset {dataset} has no model name\")\n",
    "        continue\n",
    "    \n",
    "    if dataset_model_class is not None:\n",
    "        \n",
    "        # Load the dataset with the smote parameter (e.g. False or 'auto')and random state\n",
    "        \n",
    "        smote = datasets[dataset]['smote']\n",
    "        cleaned_data = dataset_model_class(smote=smote, random_state=random_state)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Dataset {dataset} ({dataset_model_name}) not found\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # Split the data into target variable and featurest\n",
    "    \n",
    "    X = cleaned_data['X']\n",
    "    y = cleaned_data['y']\n",
    "        \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = cleaned_data.train_test_split(X, y)\n",
    "    \n",
    "    \n",
    "    # Train all the models on the dataset\n",
    "    for pred_model in datasets[dataset]['pred_models']:\n",
    "        \n",
    "        print(f\"Running {pred_model} on {dataset}\")\n",
    "        \n",
    "        # Set the steps and parameters for the pipeline\n",
    "        pipe_params = {}\n",
    "\n",
    "        match pred_model:\n",
    "            \n",
    "            case 'rf':\n",
    "                steps = [('rf', RandomForestClassifier())]\n",
    "                pipe_params.update({\n",
    "                    'rf__n_estimators': [100, 200, 300],\n",
    "                    'rf__max_depth': [2, 4, 8, 10, 20],\n",
    "                    'rf__min_samples_split': [2, 5, 10],\n",
    "                    'rf__min_samples_leaf': [1, 2, 4]\n",
    "                })\n",
    "                \n",
    "            case 'rbf_svm':\n",
    "                steps = [\n",
    "                        ('scaler', StandardScaler()),\n",
    "                        ('rbf_svm', SVC())]\n",
    "                pipe_params.update({\n",
    "                    'rbf_svm__C': [0.1, 1, 10, 100],\n",
    "                    'rbf_svm__gamma': [1, 0.1, 0.01, 0.001],\n",
    "                    'rbf_svm__kernel': ['rbf']\n",
    "                })\n",
    "                \n",
    "            case 'dnn':\n",
    "                steps = [('dnn', MLPClassifier())]\n",
    "                pipe_params.update({\n",
    "                    'dnn__hidden_layer_sizes': [(100,)],\n",
    "                    'dnn__activation': ['relu', 'tanh'],\n",
    "                    'dnn__alpha': [0.0001, 0.001, 0.01],\n",
    "                    'dnn__learning_rate': ['constant', 'adaptive']\n",
    "                })\n",
    "                    \n",
    "                \n",
    "            case _:\n",
    "                steps = None\n",
    "                print(f\"Model {pred_model} not found\")\n",
    "                \n",
    "        # Apply override parameters\n",
    "        if datasets[dataset]['pred_models'][pred_model] is not None or datasets[dataset]['pred_models'][pred_model] != {}:\n",
    "            pipe_params.update(datasets[dataset]['pred_models'][pred_model])\n",
    "                \n",
    "        pipe = Pipeline(steps=steps)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        search = GridSearchCV(pipe, pipe_params, cv=2, n_jobs=n_jobs, verbose=1)\n",
    "        \n",
    "        cluster = LocalCluster()  \n",
    "        client = Client(cluster) \n",
    "        \n",
    "        with joblib.parallel_backend(\"dask\", scatter=[X_train, y_train]):  \n",
    "            search.fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "        # Create a folder with the datetime if it doesn't exist\n",
    "        base = output + '/' + now_format\n",
    "        if not os.path.exists(base):\n",
    "            os.makedirs(base)\n",
    "        \n",
    "        # Save the model to the output folder with the dataset name and the model name\n",
    "        joblib.dump(search.best_estimator_, f\"{base}/{dataset}_{pred_model}.joblib\")\n",
    "        \n",
    "        # Log the evaluation metrics to a log file, including the best parameters\n",
    "        log += helpers.dataset_log(search, X_test, y_test, pred_model, cleaned_data.get_labels())\n",
    "    \n",
    "        \n",
    "    # Save the log to a file\n",
    "    with open(f\"{base}/{dataset}_model_comparison.txt\", 'w') as file:\n",
    "        file.write(log)\n",
    "        file.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
