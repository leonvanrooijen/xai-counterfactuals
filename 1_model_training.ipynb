{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import all datasets except boilerplate from data folder\n",
    "\n",
    "from data.datasets import CreditScore, StudentAddiction, Thyroid\n",
    "\n",
    "# Import all the necessary libraries\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from data.boilerplate import Dataset as DatasetBoilerplate\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Import json and make a small helper function to keep the code clean\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "def dataset_log(search: GridSearchCV, X_test, y_test, pred_model, labels):\n",
    "        \n",
    "    best_model = search.best_estimator_\n",
    "    best_params = search.best_params_\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Use a single formatted string instead of incremental additions\n",
    "    \n",
    "    log_model = f\"\"\"\n",
    "    MODEL {pred_model}:\n",
    "    Best parameters: {best_params}\n",
    "    Training score: {search.best_score_}\n",
    "    Test score: {search.score(X_test, y_test)}\n",
    "    \n",
    "    Classification report:\n",
    "    {classification_report(y_test, y_pred, labels=labels)}\n",
    "    Accuracy: {accuracy_score(y_test, y_pred)}\n",
    "    \n",
    "    Confusion matrix:\n",
    "    {confusion_matrix(y_test, y_pred, labels=labels)}\n",
    "    \n",
    "    Labels: {labels}\n",
    "    \"\"\"\n",
    "    return log_model\n",
    "\n",
    "\n",
    "output = 'trained_models' # Output folder for trained models\n",
    "random_state = 42 # Random state for reproducibility\n",
    "n_jobs = 4 # Number of cores to use for parallel processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here it is possible to adjust the datasets and models that will be used in the project\n",
    "# Set any of the pred_models to None if you don't want to use it\n",
    "\n",
    "# Load datasets from training_settings.json\n",
    "\n",
    "datasets = load_json_file('training_settings.json')\n",
    "\n",
    "# e.g. if you want to remove the random_forest model from the breast_cancer dataset\n",
    "# datasets['breast_cancer']['pred_models']['random_forest'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running rf on credit_score\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######################################\n",
    "### Run the models on the datasets ###\n",
    "######################################\n",
    "\n",
    "# Loop through all the datasets\n",
    "\n",
    "# Record the current date and time to the nearest minute\n",
    "now_format = datetime.now().strftime(\"%d_%m_%Y__%H-%M\")\n",
    "\n",
    "datasets = load_json_file('training_settings.json')\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    log = f\"Model training evaluation for {dataset} on {now_format}\"\n",
    "    \n",
    "    \n",
    "    # Get the model name from the dataset and load it, if it exists\n",
    "    dataset_model_name = datasets[dataset]['data_model']\n",
    "    smote = datasets[dataset]['smote']\n",
    "    \n",
    "    # Smote is applied to the data if it is set to True in the datase\n",
    "    \n",
    "    if dataset_model_name in globals():\n",
    "        \n",
    "        cleaned_data = globals()[dataset_model_name](smote=smote)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Dataset {dataset} ({dataset_model_name}) not found\")\n",
    "        break\n",
    "    \n",
    "    \n",
    "    # Split the data into target variable and featurest\n",
    "    \n",
    "    X = cleaned_data['X']\n",
    "    y = cleaned_data['y']\n",
    "        \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = cleaned_data.train_test_split(X, y)\n",
    "    \n",
    "    \n",
    "    # Train all the models on the dataset\n",
    "    for pred_model in datasets[dataset]['pred_models']:\n",
    "        \n",
    "        print(f\"Running {pred_model} on {dataset}\")\n",
    "        \n",
    "        # Set the steps and parameters for the pipeline\n",
    "        pipe_params = {}\n",
    "\n",
    "        match pred_model:\n",
    "            \n",
    "            case 'rf':\n",
    "                steps = [('rf', RandomForestClassifier())]\n",
    "                pipe_params.update({\n",
    "                    'rf__n_estimators': [100, 200, 300],\n",
    "                    'rf__max_depth': [None, 2, 4, 8, 10],\n",
    "                    'rf__min_samples_split': [2, 5, 10],\n",
    "                    'rf__min_samples_leaf': [1, 2, 4]\n",
    "                })\n",
    "                \n",
    "            case 'rbf_svm':\n",
    "                steps = [\n",
    "                        ('scaler', StandardScaler()),\n",
    "                        ('rbf_svm', SVC())]\n",
    "                pipe_params.update({\n",
    "                    'rbf_svm__C': [0.1, 1, 10, 100],\n",
    "                    'rbf_svm__gamma': [1, 0.1, 0.01, 0.001],\n",
    "                    'rbf_svm__kernel': ['rbf']\n",
    "                })\n",
    "                \n",
    "            case 'dnn':\n",
    "                model = MLPClassifier()\n",
    "                pipe_params.update({\n",
    "                    'hidden_layer_sizes': [(100,)],\n",
    "                    'activation': ['relu', 'tanh'],\n",
    "                    'alpha': [0.0001, 0.001, 0.01],\n",
    "                    'learning_rate': ['constant', 'adaptive']\n",
    "                })\n",
    "                    \n",
    "                \n",
    "            case _:\n",
    "                steps = None\n",
    "                print(f\"Model {pred_model} not found\")\n",
    "                \n",
    "        # Apply override parameters\n",
    "        pipe_params.update(datasets[dataset]['pred_models'][pred_model])\n",
    "                \n",
    "        pipe = Pipeline(steps=steps)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        search = GridSearchCV(pipe, pipe_params, n_jobs=n_jobs)\n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "        # Create a folder with the datetime if it doesn't exist\n",
    "        base = output + '/' + now_format\n",
    "        if not os.path.exists(base):\n",
    "            os.makedirs(base)\n",
    "        \n",
    "        # Save the model to the output folder with the dataset name and the model name\n",
    "        joblib.dump(search.best_estimator_, f\"{base}/{dataset}_{pred_model}.joblib\")\n",
    "        \n",
    "        # Log the evaluation metrics to a log file, including the best parameters\n",
    "        log += dataset_log(search, X_test, y_test, pred_model, cleaned_data.get_labels())\n",
    "    \n",
    "        \n",
    "    # Save the log to a file\n",
    "    with open(f\"{base}/{dataset}_model_comparison.txt\", 'w') as file:\n",
    "        file.write(log)\n",
    "        file.close()\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
